{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import urllib\n",
    "import feedparser\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSON_FILE = 'arxiv_papers.json'\n",
    "DATA_FOLDER = '../data'\n",
    "PDF_FOLDER = '../data/pdfs/'\n",
    "OUTPUT_JSON_FILEPATH = os.path.join(DATA_FOLDER, OUTPUT_JSON_FILE)\n",
    "DF_PDF_CSV_FILE = \"arxiv_papers_with_pdfs.csv\"\n",
    "DF_PDF_CSV_FILEPATH = os.path.join(PDF_FOLDER, DF_PDF_CSV_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlaod Arxiv Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arxiv_papers(query: str=\"natural language processing and large language models\", \n",
    "                          max_results: int=100, \n",
    "                          output_dir: str=DATA_FOLDER, \n",
    "                          output_json_file: str=OUTPUT_JSON_FILE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads the latest papers from the arXiv with the given query and saves them as a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for the arXiv. Default is \"natural language processing and large language models\".\n",
    "        max_results (int): The maximum number of papers to download. Default is 100.\n",
    "        output_dir (str): The directory where the JSON file will be saved. Default is DATA_FOLDER.\n",
    "        output_json_file (str): The name of the JSON file to save. Default is OUTPUT_JSON_FILE.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the downloaded papers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL encode the query parameter to replace spaces and special characters\n",
    "        encoded_query = urllib.parse.quote(query)\n",
    "        feed_url = f\"http://export.arxiv.org/api/query?search_query=all:{encoded_query}&start=0&max_results={max_results}\"\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        \n",
    "        # Initialize list to store the paper data\n",
    "        papers = []\n",
    "        \n",
    "        # Loop through each entry in the feed\n",
    "        for entry in feed.entries:\n",
    "            # Safely extract data with .get() method and set defaults if missing\n",
    "            title = entry.get(\"title\", \"No title available\")\n",
    "            # sanitized_title = sanitize_filename(title) \n",
    "            summary = entry.get(\"summary\", \"No summary available\")\n",
    "            authors = [author.name for author in entry.get(\"authors\", [])]  # List comprehension with default empty list\n",
    "            url = entry.get(\"link\", \"No URL available\")\n",
    "            pdf_link = next((link.href for link in entry.get(\"links\", []) if link.get(\"title\") == \"pdf\"), \"No PDF link available\")\n",
    "            published = entry.get(\"published\", \"No publication date available\")\n",
    "            arxiv_id = entry.get(\"id\", \"\").split('/')[-1] if \"id\" in entry else \"No arXiv ID available\"\n",
    "            \n",
    "            # Store each paper's details in a dictionary\n",
    "            paper = {\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"authors\": authors,\n",
    "                \"url\": url,\n",
    "                \"pdf_link\": pdf_link,\n",
    "                \"published\": published,\n",
    "                \"arxiv_id\": arxiv_id\n",
    "            }\n",
    "            papers.append(paper)\n",
    "        \n",
    "        # Convert to a DataFrame\n",
    "        df = pd.DataFrame(papers)\n",
    "        \n",
    "        # check if the output_json_file exists or not, also the directoies in the path\n",
    "        if not os.path.exists(output_json_file):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_dir, output_json_file)\n",
    "\n",
    "        # Save to JSON\n",
    "        with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(papers, json_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Data saved to {output_json_file}\")\n",
    "    \n",
    "        return df\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data\\arxiv_papers.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>published</th>\n",
       "      <th>arxiv_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cedille: A large autoregressive French languag...</td>\n",
       "      <td>Scaling up the size and training of autoregres...</td>\n",
       "      <td>[Martin M端ller, Florian Laurent]</td>\n",
       "      <td>http://arxiv.org/abs/2202.03371v1</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03371v1</td>\n",
       "      <td>2022-02-07T17:40:43Z</td>\n",
       "      <td>2202.03371v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Precis of Language Models are not Models of ...</td>\n",
       "      <td>Natural Language Processing is one of the lead...</td>\n",
       "      <td>[Csaba Veres]</td>\n",
       "      <td>http://arxiv.org/abs/2205.07634v1</td>\n",
       "      <td>http://arxiv.org/pdf/2205.07634v1</td>\n",
       "      <td>2022-05-16T12:50:58Z</td>\n",
       "      <td>2205.07634v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>[Kebing Jin, Hankz Hankui Zhuo]</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>2022-02-15T02:19:09Z</td>\n",
       "      <td>2202.07138v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multilingual Text Classification for Dravidian...</td>\n",
       "      <td>As the fourth largest language family in the w...</td>\n",
       "      <td>[Xiaotian Lin, Nankai Lin, Kanoksak Wattanacho...</td>\n",
       "      <td>http://arxiv.org/abs/2112.01705v1</td>\n",
       "      <td>http://arxiv.org/pdf/2112.01705v1</td>\n",
       "      <td>2021-12-03T04:26:49Z</td>\n",
       "      <td>2112.01705v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PersianLLaMA: Towards Building First Persian L...</td>\n",
       "      <td>Despite the widespread use of the Persian lang...</td>\n",
       "      <td>[Mohammad Amin Abbasi, Arash Ghafouri, Mahdi F...</td>\n",
       "      <td>http://arxiv.org/abs/2312.15713v1</td>\n",
       "      <td>http://arxiv.org/pdf/2312.15713v1</td>\n",
       "      <td>2023-12-25T12:48:55Z</td>\n",
       "      <td>2312.15713v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Cedille: A large autoregressive French languag...   \n",
       "1  A Precis of Language Models are not Models of ...   \n",
       "2  Integrating AI Planning with Natural Language ...   \n",
       "3  Multilingual Text Classification for Dravidian...   \n",
       "4  PersianLLaMA: Towards Building First Persian L...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Scaling up the size and training of autoregres...   \n",
       "1  Natural Language Processing is one of the lead...   \n",
       "2  Natural language processing (NLP) aims at inve...   \n",
       "3  As the fourth largest language family in the w...   \n",
       "4  Despite the widespread use of the Persian lang...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                   [Martin M端ller, Florian Laurent]   \n",
       "1                                      [Csaba Veres]   \n",
       "2                    [Kebing Jin, Hankz Hankui Zhuo]   \n",
       "3  [Xiaotian Lin, Nankai Lin, Kanoksak Wattanacho...   \n",
       "4  [Mohammad Amin Abbasi, Arash Ghafouri, Mahdi F...   \n",
       "\n",
       "                                 url                           pdf_link  \\\n",
       "0  http://arxiv.org/abs/2202.03371v1  http://arxiv.org/pdf/2202.03371v1   \n",
       "1  http://arxiv.org/abs/2205.07634v1  http://arxiv.org/pdf/2205.07634v1   \n",
       "2  http://arxiv.org/abs/2202.07138v2  http://arxiv.org/pdf/2202.07138v2   \n",
       "3  http://arxiv.org/abs/2112.01705v1  http://arxiv.org/pdf/2112.01705v1   \n",
       "4  http://arxiv.org/abs/2312.15713v1  http://arxiv.org/pdf/2312.15713v1   \n",
       "\n",
       "              published      arxiv_id  \n",
       "0  2022-02-07T17:40:43Z  2202.03371v1  \n",
       "1  2022-05-16T12:50:58Z  2205.07634v1  \n",
       "2  2022-02-15T02:19:09Z  2202.07138v2  \n",
       "3  2021-12-03T04:26:49Z  2112.01705v1  \n",
       "4  2023-12-25T12:48:55Z  2312.15713v1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the papers and return the DataFrame\n",
    "df = download_arxiv_papers(query=\"natural language processing and large language models\", max_results=100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download paper PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(name: str, max_length: int=255) -> str:\n",
    "    \"\"\"\n",
    "    Sanitizes a filename by replacing characters that are not allowed in filenames.\n",
    "    Args:\n",
    "        name (str): The filename to sanitize.\n",
    "        max_length (int): The maximum length of the sanitized filename. Default is 255.\n",
    "    Returns:\n",
    "        str: The sanitized filename.\n",
    "    \"\"\"\n",
    "    # Remove or replace characters not allowed in filenames\n",
    "    name = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', '_', name)  # Replace forbidden characters with underscore\n",
    "    name = name.replace(\" \", \"_\")  # Optionally replace spaces with underscores\n",
    "    # Replace all dots with underscores, except for the final one before 'pdf'\n",
    "    if '.' in name:\n",
    "        name_parts = name.rsplit('.', 1)  # Split at the last period\n",
    "        name = name_parts[0].replace('.', '_') + ('.' + name_parts[1] if len(name_parts) > 1 else '')\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    name = name.strip()\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    name = name.strip()\n",
    "\n",
    "    # Limit filename length (255 characters is a common max limit)\n",
    "    if len(name) > max_length:\n",
    "        name = name[:max_length].rstrip(\"_\")  # Trim to max length and remove trailing underscores\n",
    "\n",
    "    return name\n",
    "\n",
    "def remove_dot_from_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes the dot from the filename if it exists at the end.\n",
    "    Args:\n",
    "        name (str): The filename to remove the dot from.\n",
    "    Returns:\n",
    "        str: The filename with the dot removed, if it exists. Otherwise, the original filename.\n",
    "    \"\"\"\n",
    "    # Replace all dots with underscores, except for the final one before 'pdf'\n",
    "    if '.' in name:\n",
    "        name_parts = name.rsplit('.', 1)  # Split at the last period\n",
    "        name = name_parts[0].replace('.', '_') + ('.' + name_parts[1] if len(name_parts) > 1 else '')\n",
    "    return name\n",
    "\n",
    "def download_pdfs(df, download_dir=PDF_FOLDER) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads the PDFs from the provided DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the paper data.\n",
    "        download_dir (str): The directory where the PDFs will be saved. Default is PDF_FOLDER.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the downloaded PDFs added as a new column.\n",
    "    \"\"\"\n",
    "    # Ensure the download directory exists\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    pdf_file_names = []  # List to store PDF file names for each paper\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_url = row[\"pdf_link\"]\n",
    "        # paper_title = row[\"title\"].replace(\" \", \"_\").replace(\"/\", \"-\")  # Clean title for filename\n",
    "        paper_title = sanitize_filename(row[\"title\"]) # Clean title for filename\n",
    "\n",
    "        # Define the PDF file path based on the title and download directory\n",
    "        # pdf_file_path = os.path.join(download_dir, f\"{paper_title}.pdf\")\n",
    "        # name = (pdf_url.split('/')[-1]).replace('.', '_')\n",
    "        \n",
    "        pdf_file_path = os.path.join(download_dir, pdf_url.split('/')[-1]) + '.pdf'\n",
    "        # print(pdf_url)\n",
    "        # print(name)\n",
    "        # print(pdf_file_path)\n",
    "        \n",
    "        try:\n",
    "            # Check if the PDF already exists to avoid redundant downloads\n",
    "            if os.path.exists(pdf_file_path):\n",
    "                print(f\"PDF already exists for: {row['title']}\")\n",
    "                pdf_file_names.append(os.path.basename(pdf_file_path))\n",
    "                continue\n",
    "\n",
    "            # Download the PDF content\n",
    "            response = requests.get(pdf_url, stream=True)\n",
    "            response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "            # Write the content to a file\n",
    "            with open(pdf_file_path, \"wb\") as pdf_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:  # Filter out keep-alive new chunks\n",
    "                        pdf_file.write(chunk)\n",
    "            print(f\"Downloaded PDF for: {row['title']}\")\n",
    "            pdf_file_names.append(os.path.basename(pdf_file_path))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download PDF for {row['title']}: {e}\")\n",
    "            pdf_file_names.append(\"Download failed\")  # Log failure in the file name list\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            pdf_file_names.append(\"Download failed\")  # Log generic failure\n",
    "    # Add the PDF file names to the DataFrame\n",
    "    df[\"pdf_file_name\"] = pdf_file_names\n",
    "\n",
    "    # Save the updated DataFrame (optional, if needed for further use)\n",
    "    df.to_csv(os.path.join(download_dir, DF_PDF_CSV_FILE), index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF for: Cedille: A large autoregressive French language model\n",
      "Downloaded PDF for: A Precis of Language Models are not Models of Language\n",
      "Downloaded PDF for: Integrating AI Planning with Natural Language Processing: A Combination\n",
      "  of Explicit and Tacit Knowledge\n",
      "Downloaded PDF for: Multilingual Text Classification for Dravidian Languages\n",
      "Downloaded PDF for: PersianLLaMA: Towards Building First Persian Large Language Model\n",
      "Downloaded PDF for: How Good are Commercial Large Language Models on African Languages?\n",
      "Downloaded PDF for: Large language models in bioinformatics: applications and perspectives\n",
      "Downloaded PDF for: Large Language Models are not Models of Natural Language: they are\n",
      "  Corpus Models\n",
      "Downloaded PDF for: Multilingual Brain Surgeon: Large Language Models Can be Compressed\n",
      "  Leaving No Language Behind\n",
      "Downloaded PDF for: Benchmarking Language Models for Code Syntax Understanding\n",
      "Downloaded PDF for: JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset\n",
      "Downloaded PDF for: Scientific Computing with Large Language Models\n",
      "Downloaded PDF for: Natural Language Processing using Hadoop and KOSHIK\n",
      "Downloaded PDF for: Do Large Language Models Speak All Languages Equally? A Comparative\n",
      "  Study in Low-Resource Settings\n",
      "Downloaded PDF for: Large Language Models on the Chessboard: A Study on ChatGPT's Formal\n",
      "  Language Comprehension and Complex Reasoning Skills\n",
      "Downloaded PDF for: Formal Aspects of Language Modeling\n",
      "Downloaded PDF for: Improving Arithmetic Reasoning Ability of Large Language Models through\n",
      "  Relation Tuples, Verification and Dynamic Feedback\n",
      "Downloaded PDF for: HinFlair: pre-trained contextual string embeddings for pos tagging and\n",
      "  text classification in the Hindi language\n",
      "Downloaded PDF for: Spontaneous Emerging Preference in Two-tower Language Model\n",
      "Downloaded PDF for: The LLM Language Network: A Neuroscientific Approach for Identifying\n",
      "  Causally Task-Relevant Units\n",
      "Downloaded PDF for: From BERT to GPT-3 Codex: Harnessing the Potential of Very Large\n",
      "  Language Models for Data Management\n",
      "Downloaded PDF for: Unnatural Language Processing: Bridging the Gap Between Synthetic and\n",
      "  Natural Language Data\n",
      "Downloaded PDF for: Efficiently Adapting Pretrained Language Models To New Languages\n",
      "Downloaded PDF for: Self Generated Wargame AI: Double Layer Agent Task Planning Based on\n",
      "  Large Language Model\n",
      "Downloaded PDF for: Knowledge Engineering using Large Language Models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>published</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cedille: A large autoregressive French languag...</td>\n",
       "      <td>Scaling up the size and training of autoregres...</td>\n",
       "      <td>[Martin M端ller, Florian Laurent]</td>\n",
       "      <td>http://arxiv.org/abs/2202.03371v1</td>\n",
       "      <td>http://arxiv.org/pdf/2202.03371v1</td>\n",
       "      <td>2022-02-07T17:40:43Z</td>\n",
       "      <td>2202.03371v1</td>\n",
       "      <td>2202.03371v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Precis of Language Models are not Models of ...</td>\n",
       "      <td>Natural Language Processing is one of the lead...</td>\n",
       "      <td>[Csaba Veres]</td>\n",
       "      <td>http://arxiv.org/abs/2205.07634v1</td>\n",
       "      <td>http://arxiv.org/pdf/2205.07634v1</td>\n",
       "      <td>2022-05-16T12:50:58Z</td>\n",
       "      <td>2205.07634v1</td>\n",
       "      <td>2205.07634v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrating AI Planning with Natural Language ...</td>\n",
       "      <td>Natural language processing (NLP) aims at inve...</td>\n",
       "      <td>[Kebing Jin, Hankz Hankui Zhuo]</td>\n",
       "      <td>http://arxiv.org/abs/2202.07138v2</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07138v2</td>\n",
       "      <td>2022-02-15T02:19:09Z</td>\n",
       "      <td>2202.07138v2</td>\n",
       "      <td>2202.07138v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multilingual Text Classification for Dravidian...</td>\n",
       "      <td>As the fourth largest language family in the w...</td>\n",
       "      <td>[Xiaotian Lin, Nankai Lin, Kanoksak Wattanacho...</td>\n",
       "      <td>http://arxiv.org/abs/2112.01705v1</td>\n",
       "      <td>http://arxiv.org/pdf/2112.01705v1</td>\n",
       "      <td>2021-12-03T04:26:49Z</td>\n",
       "      <td>2112.01705v1</td>\n",
       "      <td>2112.01705v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PersianLLaMA: Towards Building First Persian L...</td>\n",
       "      <td>Despite the widespread use of the Persian lang...</td>\n",
       "      <td>[Mohammad Amin Abbasi, Arash Ghafouri, Mahdi F...</td>\n",
       "      <td>http://arxiv.org/abs/2312.15713v1</td>\n",
       "      <td>http://arxiv.org/pdf/2312.15713v1</td>\n",
       "      <td>2023-12-25T12:48:55Z</td>\n",
       "      <td>2312.15713v1</td>\n",
       "      <td>2312.15713v1.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Cedille: A large autoregressive French languag...   \n",
       "1  A Precis of Language Models are not Models of ...   \n",
       "2  Integrating AI Planning with Natural Language ...   \n",
       "3  Multilingual Text Classification for Dravidian...   \n",
       "4  PersianLLaMA: Towards Building First Persian L...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Scaling up the size and training of autoregres...   \n",
       "1  Natural Language Processing is one of the lead...   \n",
       "2  Natural language processing (NLP) aims at inve...   \n",
       "3  As the fourth largest language family in the w...   \n",
       "4  Despite the widespread use of the Persian lang...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                   [Martin M端ller, Florian Laurent]   \n",
       "1                                      [Csaba Veres]   \n",
       "2                    [Kebing Jin, Hankz Hankui Zhuo]   \n",
       "3  [Xiaotian Lin, Nankai Lin, Kanoksak Wattanacho...   \n",
       "4  [Mohammad Amin Abbasi, Arash Ghafouri, Mahdi F...   \n",
       "\n",
       "                                 url                           pdf_link  \\\n",
       "0  http://arxiv.org/abs/2202.03371v1  http://arxiv.org/pdf/2202.03371v1   \n",
       "1  http://arxiv.org/abs/2205.07634v1  http://arxiv.org/pdf/2205.07634v1   \n",
       "2  http://arxiv.org/abs/2202.07138v2  http://arxiv.org/pdf/2202.07138v2   \n",
       "3  http://arxiv.org/abs/2112.01705v1  http://arxiv.org/pdf/2112.01705v1   \n",
       "4  http://arxiv.org/abs/2312.15713v1  http://arxiv.org/pdf/2312.15713v1   \n",
       "\n",
       "              published      arxiv_id     pdf_file_name  \n",
       "0  2022-02-07T17:40:43Z  2202.03371v1  2202.03371v1.pdf  \n",
       "1  2022-05-16T12:50:58Z  2205.07634v1  2205.07634v1.pdf  \n",
       "2  2022-02-15T02:19:09Z  2202.07138v2  2202.07138v2.pdf  \n",
       "3  2021-12-03T04:26:49Z  2112.01705v1  2112.01705v1.pdf  \n",
       "4  2023-12-25T12:48:55Z  2312.15713v1  2312.15713v1.pdf  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_pdfs = download_pdfs(df)\n",
    "df_with_pdfs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
